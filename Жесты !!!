import cv2
import mediapipe as mp
import numpy as np
import pyttsx3 as pt


tts = pt.init()


mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
mp_hands = mp.solutions.hands


cap = cv2.VideoCapture(0)

with mp_hands.Hands(
        model_complexity=0,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5) as hands:
  while True:
    success, image = cap.read()

    image.flags.writeable = False
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands.process(image)

    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS,
            mp_drawing_styles.get_default_hand_landmarks_style(),
            mp_drawing_styles.get_default_hand_connections_style())

        cv2.imshow('x', cv2.flip(image, 1))

    gesture = 'none'
    text = ''
    if #программа видит такой-то знак#:
        gesture = #какому-то слову/букве

    search=True

    if search:
        if gesture == '#какому-то слову':
            tts.say(gesture)
            tts.runAndWait()

        if gesture == '#какой-то букве':
            text.append('#та самая буква')
            if #жестов не было больше 3ёх секунд:
                tts.say(text)
                tts.runAndWait()


    if cv2.waitKey(5) & 0xFF == ord(' '):
        break
